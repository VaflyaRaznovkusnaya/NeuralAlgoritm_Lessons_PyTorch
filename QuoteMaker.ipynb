{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/NIDFUPZskY0SQSvFgQOi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VaflyaRaznovkusnaya/NeuralAlgoritm_Lessons_PyTorch/blob/main/QuoteMaker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyjiW9qCEoc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55e1cf9b-3335-4500-b450-26a376376177"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' ', 'e', 't', 'a', 'n', 'o', 'h', 'i', 's', 'r', 'd', 'l', '\\n', 'u', 'c', 'y', 'm', '.', 'g', 'w', 'p', 'f', ',', 'b', 'k', '“', '”', 'v', 'N', '’', 'I', 'M', 'T', 'H', 'S', 'W', 'A', 'D', 'E', '!', '?', 'J', 'x', 'B', 'G', 'j', '-', 'L', 'q', '_', 'z', 'C', '—', 'Y', 'O', 'P', 'F', 'R', '‘', ':', 'é', 'Q', 'U', '[', ']', 'V', '0', ';', '3', '1', 'K', '7', '\\ufeff', '4', '8', '5', '2', '(', ')', '6']\n",
            "Loss: 2.679091901779175\n",
            " and and the the the to the wore and and the the the wore the the the the sher the the to the the she there and warly the the were the solen the the to can the the was and and the had the sored the wan\n",
            "Loss: 2.021087739467621\n",
            " and she was the was and the cant. “The plomested the store and the dinged the lecing the call the she said the conged the canced the conged she she and the to and singed the said was a to the conced a\n",
            "Loss: 1.8043387842178344\n",
            " a store the can of the inn the good the man from her firls and a slile a promite a can the was and the are was and the reverting a lile and the the a for a disal said the to her the said the rooper th\n",
            "Loss: 1.6756747102737426\n",
            " and the good and Helen was the are the said. “I’m the that Carmised the tround the parting the to the tore was the that the can and the conter to the tore and her case the porstround her for the can w\n",
            "Loss: 1.6034209060668945\n",
            "                                                                                                  it she said she was diving she was to the probage to the stared to the store the discors was thing the \n",
            "Loss: 1.5374062633514405\n",
            " a been the called a startled and her and the care and the reaced a startled a plase a said the been went had said the been the said the down and her care was the girls and the said a startled the disc\n",
            "Loss: 1.4903810811042786\n",
            "                                                                                                                                                                                                         \n",
            "Loss: 1.4574011087417602\n",
            "                                                                                                                                                                                                         \n",
            "Loss: 1.419409761428833\n",
            "                                                                                                                                                                                                         \n",
            "Loss: 1.3925488901138305\n",
            " and the sub to probably that the startled her discovered the inn and the thief, and seemed the disconds and can with the place with the startled to the street was to her father and seen and for the st\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "#/////////\n",
        "# Sozdaem class neironki\n",
        "\n",
        "class TextAnalize(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, embedding_size, n_layers=1):\n",
        "        super(TextAnalize, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.encoder = nn.Embedding(self.input_size, self.embedding_size)\n",
        "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, self.n_layers)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(self.hidden_size, self.input_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.encoder(x).squeeze(2)\n",
        "        out, (ht1, ct1) = self.lstm(x, hidden)\n",
        "        out = self.dropout(out)\n",
        "        x = self.fc(out)\n",
        "        return x, (ht1, ct1)\n",
        "\n",
        "    def init_hidden(self, batch_size=1):\n",
        "        return (torch.zeros(self.n_layers, batch_size, self.hidden_size, requires_grad=True).to(device),\n",
        "               torch.zeros(self.n_layers, batch_size, self.hidden_size, requires_grad=True).to(device))\n",
        "\n",
        "\n",
        "# ------------------------- Podgotovka dannih\n",
        "\n",
        "TRAIN_TEXT_FILE_PATH = 'LiliacINC.txt'\n",
        "with open(TRAIN_TEXT_FILE_PATH) as text_file:\n",
        "    text_sample = text_file.readlines()\n",
        "text_sample = ' '.join(text_sample)\n",
        "\n",
        "def text_to_seq(text_sample):\n",
        "    char_counts = Counter(text_sample)\n",
        "    char_counts = sorted(char_counts.items(), key = lambda x: x[1], reverse=True)\n",
        "\n",
        "    sorted_chars = [char for char, _ in char_counts]\n",
        "    print(sorted_chars)\n",
        "    char_to_idx = {char: index for index, char in enumerate(sorted_chars)}\n",
        "    idx_to_char = {v: k for k, v in char_to_idx.items()}\n",
        "    sequence = np.array([char_to_idx[char] for char in text_sample])\n",
        "\n",
        "    return sequence, char_to_idx, idx_to_char\n",
        "\n",
        "#Make them beforehand\n",
        "sequence, char_to_idx, idx_to_char = text_to_seq(text_sample)\n",
        "\n",
        "#------------------------ Generiruem batchi\n",
        "\n",
        "SEQ_LEN = 256\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "def get_batch(sequence):\n",
        "    trains = []\n",
        "    targets = []\n",
        "    for _ in range(BATCH_SIZE):\n",
        "        batch_start = np.random.randint(0, len(sequence) - SEQ_LEN)\n",
        "        chunk = sequence[batch_start: batch_start + SEQ_LEN]\n",
        "        train = torch.LongTensor(chunk[:-1]).view(-1, 1)\n",
        "        target = torch.LongTensor(chunk[1:]).view(-1, 1)\n",
        "        trains.append(train)\n",
        "        targets.append(target)\n",
        "    return torch.stack(trains, dim=0), torch.stack(targets, dim=0)\n",
        "\n",
        "#------------------------- Generiruem text\n",
        "\n",
        "def evaluate(model, char_to_idx, idx_to_char, start_text=' ', prediction_len=200, temp=0.3):\n",
        "    hidden = model.init_hidden()\n",
        "    idx_input = [char_to_idx[char] for char in start_text]\n",
        "    train = torch.LongTensor(idx_input).view(-1, 1, 1).to(device)\n",
        "    predicted_text = start_text\n",
        "\n",
        "    _, hidden = model(train, hidden)\n",
        "\n",
        "    inp = train[-1].view(-1, 1, 1)\n",
        "\n",
        "    for i in range(prediction_len):\n",
        "        output, hidden = model(inp.to(device), hidden)\n",
        "        output_logits = output.cpu().data.view(-1)\n",
        "        p_next = F.softmax(output_logits / temp, dim=-1).detach().cpu().data.numpy()\n",
        "        top_index = np.random.choice(len(char_to_idx), p=p_next)\n",
        "        inp = torch.LongTensor([top_index]).view(-1, 1, 1).to(device)\n",
        "        predicted_char = idx_to_char[top_index]\n",
        "        predicted_text += predicted_char\n",
        "\n",
        "    return predicted_text\n",
        "\n",
        "# ------------------------- Obuchenie\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = TextAnalize(input_size=len(idx_to_char), hidden_size=128, embedding_size=128, n_layers=2)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, amsgrad=True)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    patience=4,\n",
        "    verbose=True,\n",
        "    factor=0.5\n",
        ")\n",
        "\n",
        "n_epochs = 500\n",
        "loss_avg = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    train, target = get_batch(sequence)\n",
        "    train = train.permute(1, 0, 2).to(device)\n",
        "    target = target.permute(1, 0, 2).to(device)\n",
        "    hidden = model.init_hidden(BATCH_SIZE)\n",
        "\n",
        "    output, hidden = model(train, hidden)\n",
        "    loss = criterion(output.permute(1, 2, 0), target.squeeze(-1).permute(1, 0))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss_avg.append(loss.item())\n",
        "    if len(loss_avg) >= 50:\n",
        "        mean_loss = np.mean(loss_avg)\n",
        "        print(f'Loss: {mean_loss}')\n",
        "        scheduler.step(mean_loss)\n",
        "        loss_avg = []\n",
        "        model.eval()\n",
        "        predicted_text = evaluate(model, char_to_idx, idx_to_char)\n",
        "        print(predicted_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Generiruem text\n",
        "model.eval()\n",
        "\n",
        "print(evaluate(\n",
        "    model,\n",
        "    char_to_idx,\n",
        "    idx_to_char,\n",
        "    temp=0.5,\n",
        "    prediction_len=120,\n",
        "    start_text='fight '\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-zwNXSc-kym",
        "outputId": "5c61e685-aaf3-4cff-80b4-9e98d9db2df7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fight in a need a\n",
            " conterment and the story to sure the right, the winded.\n",
            " \n",
            " “And I step to the moment be small be mysteries \n"
          ]
        }
      ]
    }
  ]
}